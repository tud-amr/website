---
title: Perception and Planning for Mobile Manipulation in Changing Environments
---

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Perception and Planning for Mobile Manipulation in Changing Environments</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
        }
        /* header {
            background: #333;
            color: white;
            padding: 20px;
            text-align: center;
        } */
        header {
            position: relative;
            height: auto;
            min-height: 300px;
            text-align: center;
            color: white;
        }
        .header-content {
            padding: 40px 20px;  /* Reduced padding for mobile */
        }
        .header-background {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: -1;
            display: flex;
            flex-direction: row;  /* Make sure items flow horizontally */
            justify-content: space-between;
        }
        .header-image-left {
            width: 50%;  /* Slightly less than 50% to create some gap */
            height: 100%;
            background: url('/assets/images/workshops/2025_iros/bkg1.webp') no-repeat center center/cover;
            float: left;  /* Ensure left alignment */
        }
        .header-image-right {
            width: 50%;  /* Slightly less than 50% to create some gap */
            height: 100%;
            background: url('/assets/images/workshops/2025_iros/bkg2.webp') no-repeat center center/cover;
            float: right;  /* Ensure right alignment */
        }
        .overlay {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.5);
            z-index: -1;
        }
        .header-content {
            position: relative;
            padding: 100px;
        }
        nav {
            background: #444;
            padding: 10px;
            text-align: center;
            position: sticky;
            top: 0;
            z-index: 1000; /* ensures it stays above content */
        }
        nav ul {
            list-style: none;
            padding: 0;
        }
        /* nav ul li {
            display: inline;
            margin: 0 15px;
        } */
        nav ul li {
            display: inline;
            margin: 0 40px;
        }
        nav ul li a {
            color: white;
            text-decoration: none;
            font-size: 18px;
        }
        #home p, #home ul {
            width: 66%;
            margin-left: auto;
            margin-right: auto;
            /* padding-left: 20px;
            padding-right: 20px; */
        }
        #papers p, #papers ul {
            width: 66%;
            margin-left: auto;
            margin-right: auto;
            /* padding-left: 20px;
            padding-right: 20px; */
        }
        section {
            padding: 50px;
            text-align: center;
            background: white;
            margin: 20px;
            border-radius: 10px;
            box-shadow: 0px 4px 8px rgba(0,0,0,0.2);
        }
        .schedule-table {
            width: 65%;
            border-collapse: collapse;
            margin-top: 20px;
            overflow-x: auto;
            margin-left: auto;
            margin-right: auto;
            text-align: center;
            /* display: block; */
        }
        .schedule-table th, .schedule-table td {
            border: 1px solid #ddd;
            padding: 10px;
            text-align: left;
        }
        .schedule-table th {
            background-color: #333;
            color: white;
        }
        footer {
            background: #333;
            color: white;
            text-align: center;
            padding: 10px;
            position: relative;
            bottom: 0;
            width: 100%;
        }
        .profile {
            display: flex;
            align-items: center;
            justify-content: center;
            width: 66%;
            margin-left: auto;
            margin-right: auto;
            flex-wrap: wrap;
            gap: 20px;
            margin-top: 20px;
        }
        .profile-card {
            background: #fff;
            padding: 15px;
            border-radius: 10px;
            text-align: center;
            width: 320px;
            box-shadow: 0px 4px 8px rgba(0,0,0,0.2);
        }
        .profile-card img {
            width: 280;
            height: 280px;
            border-radius: 50%;
            object-fit: cover;
        }
        .profile-card a {
            text-decoration: none;
            color: #5E35B1;  /* Modern blue */
        }
        .profile-card a:hover {
            text-decoration: underline;
            /* Optional: slightly darker shade on hover */
            filter: brightness(85%);
        }
        .bio {
            display: none;
            text-align: left;
            margin-top: 10px;
        }
        .show-bio {
            color: #5E35B1;
            cursor: pointer;
        }

        ul li p {
            list-style-position: outside;
            text-indent: -10em;
            padding-left: 2em;
        }

        section ul {
            text-align: left;
            font-family: Arial, sans-serif;
            font-size: 1.2em;
        }
        

        @media screen and (max-width: 768px) {
            .header-content h1 p {
                font-size: 1.2em !important;  /* Override inline styles */
            }
            
            .header-content h1 p:nth-child(2) {
                font-size: 0.9em !important;
            }
            
            .header-content h1 p:nth-child(3) {
                font-size: 0.7em !important;
            }
            
            /* Adjust background images for mobile */
            .header-image-left, .header-image-right {
                width: 100%;
                height: 50%;
                float: none;
            }
            
            .header-background {
                flex-direction: column;  /* Stack images vertically on mobile */
            }
        }
    </style>
    <script>
        function toggleBio(id) {
            var bio = document.getElementById(id);
            if (bio.style.display === "none") {
                bio.style.display = "block";
            } else {
                bio.style.display = "none";
            }
        }
    </script>
</head>

<body>
    <header>
        <!-- <div class="header-background"></div> -->
        <div class="header-background">
            <div class="header-image-left"></div>
            <div class="header-image-right"></div>
        </div>
        <div class="overlay"></div>
        <div class="header-content">
            <h1><p style="font-size: 1.5em;">Perception and Planning for Mobile Manipulation in Changing Environments</p></h1>
            <h1><p style="font-size: 1.0em;">IROS 2025 Workshop</p></h1>
            <h1><p style="font-size: 0.8em;"> 8:00-12:00 AM, Oct. 20, 2025 </p></h1>
            <h1><p style="font-size: 0.8em;"> Venue 309, Hangzhou International Expo Center, China </p></h1>
        </div>
    </header>
    <nav>
        <ul>
            <li><a href="#home">Home</a></li>
            <li><a href="#schedule">Schedule</a></li>
            <li><a href="#speakers">Invited Speakers</a></li>
            <li><a href="#papers">Call for Papers</a></li>
            <li><a href="#organizers">Organizers</a></li>
        </ul>
    </nav>
    <section id="home">
        <h2>About the Workshop </h2>
        <p style="text-align: left; font-size: 1.2em;">  Human workers in factories, warehouses, and hospitality settings effortlessly perceive their surroundings, adapt to continuous changes, and adjust their actions based on new assignments or environmental modifications. In contrast, mobile robots often struggle with real-time perception and task and motion planning in these changing environments, limiting their ability to function effectively in real-world scenarios.</p>
        <p style="text-align: left; font-size: 1.2em;">To advance mobile manipulation, robots must <strong>continuously perceive environmental changes</strong>—such as shifted objects, human and other robot’s activities, and unforeseen obstacles—and <strong>update their task and motion plans accordingly</strong>. However, most existing research assumes relatively static environments, limiting adaptability in practical applications.</p>
        <p style="text-align: left; font-size: 1.2em;"> <strong>Key challenges remain in:</strong></p>
            <ul>
                <li>Efficient real-time perception and modeling of dynamic environments. </li>
                <li>Task and motion planning strategies that can quickly adapt to new/changing environments while ensuring smooth and effective execution.</li>
            </ul>
        <p style="text-align: left; font-size: 1.2em;">This workshop will explore techniques for efficient environment modeling and real-time task and motion planning in dynamic environments. We will also discuss how perception and planning can be tightly integrated to improve the adaptability and robustness of mobile manipulation systems in real-world settings.</p>

    </section>
    <section id="schedule">
        <h2>Schedule</h2>
        <table class="schedule-table">
            <tr>
                <th>Time</th>
                <th>Event</th>
                <th>Comments</th>
            </tr>
            <tr>
                <td>8:00 - 8:10</td>
                <td>Opening</td>
                <td> </td>
            </tr>
            <tr>
                <td>8:10 - 8:35</td>
                <td>Speaker 1</td>
                <td> </td>
            </tr>
            <tr>
                <td>8:35 - 9:00</td>
                <td>Speaker 2</td>
                <td> </td>
            </tr>
            <tr>
                <td>9:00 - 9:25</td>
                <td>Speaker 3</td>
                <td> </td>
            </tr>
            <tr>
                <td>9:25 - 9:40</td>
                <td>Spotlight Talk Videos</td>
                <td> (Quick video rounds) </td>
            </tr>
            <tr>
                <td>9:40 - 10:00</td>
                <td>Poster Session</td>
                <td> </td>
            </tr>
            <tr>
                <td>10:00 - 10:15</td>
                <td>Coffee Break</td>
                <td> </td>
            </tr>
            <tr>
                <td>10:15 - 10:40</td>
                <td>Speaker 4</td>
                <td> </td>
            </tr>
            <tr>
                <td>10:40 - 11:05</td>
                <td>Speaker 5</td>
                <td> </td>
            </tr>
            <tr>
                <td>11:05 - 11:30</td>
                <td>Speaker 6</td>
                <td> </td>
            </tr>
            <tr>
                <td>11:30 - 12:00</td>
                <td>Discussion with Audience</td>
                <td> </td>
            </tr>
        </table>
    </section>
    <section id="speakers">
        <h2>Invited Speakers</h2>
        <p>* Ranked by the first letter of name.</p>
        <div class="profile">
            <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/valada_page.jpg" alt="Speaker 1">
                <h3><a href="https://rl.uni-freiburg.de/people/valada" target="_blank"> Abhinav Valada</a></h3>
                <p>Professor, University of Freiburg</p>
                <p class="show-bio" onclick="toggleBio('bio1')">Show Bio</p>
                <p class="bio" id="bio1">Abhinav Valada is a Full Professor (W3) at the University of Freiburg, where he directs the Robot Learning Lab. He is a member of the Department of Computer Science, the BrainLinks-BrainTools center, and a founding faculty of the ELLIS unit Freiburg. Abhinav is a DFG Emmy Noether AI Fellow, Scholar of the ELLIS Society, IEEE Senior Member, and Chair of the IEEE Robotics and Automation Society Technical Committee on Robot Learning.

                    He received his PhD (summa cum laude) working with Prof. Wolfram Burgard at the University of Freiburg in 2019, his MS in Robotics from Carnegie Mellon University in 2013, and his BTech. in Electronics and Instrumentation Engineering from VIT University in 2010. After his PhD, he worked as a Postdoctoral researcher and subsequently an Assistant Professor (W1) from 2020 to 2023. He co-founded and served as the Director of Operations of Platypus LLC from 2013 to 2015, a company developing autonomous robotic boats in Pittsburgh, and has previously worked at the National Robotics Engineering Center and the Field Robotics Center of Carnegie Mellon University from 2011 to 2014.
                    
                    Abhinav’s research lies at the intersection of robotics, machine learning, and computer vision with a focus on tackling fundamental robot perception, state estimation, and planning problems to enable robots to operate reliably in complex and diverse domains. The overall goal of his research is to develop scalable lifelong robot learning systems that continuously learn multiple tasks from what they perceive and experience by interacting with the real world. For his research, he received the IEEE RAS Early Career Award in Robotics and Automation, IROS Toshio Fukuda Young Professional Award, NVIDIA Research Award, AutoSens Most Novel Research Award, among others. Many aspects of his research have been prominently featured in wider media such as the Discovery Channel, NBC News, Business Times, and The Economic Times.</p>
            </div>
            <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/Bucher.png" alt="Speaker 2">
                <h3><a href="https://bucherb.github.io/" target="_blank">Bernadette Bucher (tentative)</a></h3>
                <p>Assistant Professor, University of Michigan</p>
                <p class="show-bio" onclick="toggleBio('bio2')">Show Bio</p>
                <p class="bio" id="bio2">I am an Assistant Professor in the Robotics Department (primary) and Computer Science and Engineering Department at University of Michigan.

                    My research interests lie in the intersection of robotics, computer vision, and machine learning. My research is on learning interpretable visual representations and estimating their uncertainty for use in downstream science and robotics tasks particularly autonomous mobile manipulation.
                    
                    Previously, I have worked at the Boston Dynamics AI Institute, NVIDIA Research, and Lockheed Martin Corporation. I received my PhD in computer science in the GRASP lab at University of Pennsylvania co-advised by Dr. Kostas Daniilidis and Dr. Nikolai Matni. I received an M.A. in Mathematics, M.A. in Economics, and B.S. in Mathematics and Economics from The University of Alabama in 2014.</p>
            </div>
            <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/he_wang.jpg" alt="Speaker 3">
                <h3><a href="https://hughw19.github.io/" target="_blank">He Wang</a></h3>
                <p>Assistant Professor, Peking University</p>
                <p class="show-bio" onclick="toggleBio('bio3')">Show Bio</p>
                <p class="bio" id="bio3">I am a tenure-track assistant professor in the Center on Frontiers of Computing Studies (CFCS) at Peking University. I founded and lead the Embodied Perception and InteraCtion (EPIC) Lab with the mission of developing generalizable skills and embodied multimodal large model for robots to facilitate embodied AGI.

                    I am also the director of the PKU-Galbot joint lab of Embodied AI and the BAAI center of Embodied AI. I have published more than 50 papers in top conferences and journals of computer vision, robotics, and learning, including CVPR/ICCV/ECCV/TRO/ICRA/IROS/NeurIPS/ICLR/AAAI. My pioneering work on category-level 6D pose estimation, NOCS, received the 2022 World Artificial Intelligence Conference Youth Outstanding Paper (WAICYOP) Award, and my work also received ICCV 2023 best paper finalist, ICRA 2023 outstanding manipulation paper award finalist and Eurographics 2019 best paper honorable mention.
                    
                    I serve as an associate editor of Image and Vision Computing and serve as an area chair in CVPR 2022 and WACV 2022. Prior to joining Peking University, I received my Ph.D. degree from Stanford University in 2021 under the advisory of Prof. Leonidas J.Guibas and my Bachelor's degree from Tsinghua University in 2014.</p>
            </div>
            <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/Lerrel Pinto.jpg" alt="Speaker 4">
                <h3><a href="https://www.lerrelpinto.com/" target="_blank">Lerrel Pinto (tentative)</a></h3>
                <p>Assistant Professor, New York University</p>
                <p class="show-bio" onclick="toggleBio('bio4')">Show Bio</p>
                <p class="bio" id="bio4">I am an Assistant Professor of Computer Science at NYU Courant and part of the CILVR group. Before that, I was at UC Berkeley for a postdoc, at CMU Robotics Institute for a PhD, and at IIT Guwahati for undergrad.

                    Research: I run the General-purpose Robotics and AI Lab (GRAIL) with the goal of getting robots to generalize and adapt in the messy world we live in. Our research focuses broadly on robot learning and decision making, with an emphasis on large-scale learning (both data and models), representation learning for sensory data, developing algorithms to model actions and behavior, reinforcement learning for adapting to new scenarios, and building open-sourced affordable robots. A talk on our recent robotics efforts is here. If you are interested in joining our lab, please read this.</p>
            </div>
            <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/niko2021.png" alt="Speaker 5">
                <h3><a href="https://nikosuenderhauf.github.io/" target="_blank">Niko Suenderhauf</a></h3>
                <p>Professor, Queensland University of Technology</p>
                <p class="show-bio" onclick="toggleBio('bio5')">Show Bio</p>
                <p class="bio" id="bio5">I am a Professor at Queensland University of Technology (QUT) in Brisbane and Deputy Director of the QUT Centre for Robotics, where I lead the Visual Understanding and Learning Program.

                    I am also Deputy Director (Research) for the ARC Research Hub in Intelligent Robotic Systems for Real-Time Asset Management (2022-2027) and was Chief Investigator of the Australian Centre for Robotic Vision 2017-2020.
                    
                    I conduct research in robotic vision and robot learning, at the intersection of robotics, computer vision, and machine learning. My research interests focus on robotic learning for manipulation, interaction and navigation, scene understanding, and the reliability of deep learning for open-set and open-world conditions.</p>
            </div>
            <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/yunzhu_li.jpg" alt="Speaker 6">
                <h3><a href="https://yunzhuli.github.io/" target="_blank">Yunzhu Li (tentative)</a></h3>
                <p>Assistant Professor, Columbia University</p>
                <p class="show-bio" onclick="toggleBio('bio6')">Show Bio</p>
                <p class="bio" id="bio6">I am an Assistant Professor of Computer Science at Columbia University.

                    Before joining Columbia, I was an Assistant Professor at UIUC CS. I also spent time as a Postdoc at the Stanford Vision and Learning Lab (SVL), working with Fei-Fei Li and Jiajun Wu. I received my PhD from the Computer Science and Artificial Intelligence Laboratory (CSAIL) at MIT, where I was advised by Antonio Torralba and Russ Tedrake, and I obtained my bachelor's degree from Peking University..</p>
            </div>
        </div>
    </section>

    </section>
    <section id="papers">
        <h2>Call for Papers</h2>
        <p style="text-align: left; font-size: 1.2em;"></p>
        <p style="text-align: left; font-size: 1.2em;"></p>
        <p style="text-align: left; font-size: 1.2em;"> We invite researchers and industry professionals to submit short papers of up to four pages for presentation at our workshop on “<b>Perception and Planning for Mobile Manipulation in Changing Environments</b>”. 
            The workshop will take place at IROS 2025, Hangzhou, China, on 20th October 2025 (8:00-12:00 AM). 
            All submissions will undergo <b>single-blind peer review</b> by experts in the field. 
            Accepted submissions will be presented during <b>dedicated poster sessions</b> at the workshop and in spotlight presentations. 
            Authors are responsible for <b>printing and bringing their own posters</b> and <b>at least one author must be present</b> during their poster presentation. 
            Accepted abstracts will be <b>published on the workshop website after IROS 2025</b>, unless otherwise specified by the authors.
            All submissions should be uploaded via <a href=https://openreview.net/group?id=IEEE.org/IROS/2025/Workshop/PM2CE>OpenReview</a>.</p>
            <p style="text-align: left; font-size: 1.2em;"> Papers should be submitted before <b>Aug. 25, 2025, 23.59 PM UTC-0</b>. The notification of acceptance is send around Sept. 1, 2025.</p>
        <p style="text-align: left; font-size: 1.2em;"><b>Topics of interest include, but are not limited to:</b></p>
        <ul>
            <li>Long-term dynamic environment/object modeling </li>
            <li>Open-world environment modeling</li>
            <li>Task and motion planning for manipulation or mobile manipulation</li>
            <li>Motion planning for single or multiple mobile manipulators</li>
            <li>Inter-robot communication</li>
            <li>Human-robot/robot-robot interaction</li>
            <li>Multi-robot coordination/collaboration in manipulation tasks</li>
            <li>Mobile manipulation systems</li>
        </ul>
        <p style="text-align: left; font-size: 1.2em;"> For any questions, please feel free to contact us via email: <a href=mailto:irosworkshop.pm2ce@gmail.com>irosworkshop.pm2ce@gmail.com</a>.</p>
        <!-- <p style="text-align: left; font-size: 1.2em;">Important dates:</p>
        <ul>
            <li><p style="text-align: left; font-size: 1.2em;">Submission deadline:  Aug. 25, 2025</p></li>
            <li><p style="text-align: left; font-size: 1.2em;">Notification of acceptance:  Sept. 1, 2025</p></li>
            <li><p style="text-align: left; font-size: 1.2em;">Workshop date: Oct. 20, 2025 (8:00-12:00 AM)</p></li>
        </ul> -->
        <br>
        <p style="text-align: left; font-size: 1.2em;"><b>Detailed instructions for submission via OpenReview:</b></p>
        <ul>
            <li>Please submit your workshop paper via <a href=https://openreview.net/group?id=IEEE.org/IROS/2025/Workshop/PM2CE> OpenReview </a> </li>
            <li>The review process will follow a single-blind format. Accepted papers will be made publicly available on the workshop website, unless otherwise specified by the authors. Reviews and rejected submissions will remain confidential. For any questions or concerns, please feel free to contact the organizers at: <a href=mailto:irosworkshop.pm2ce@gmail.com>irosworkshop.pm2ce@gmail.com</a>.</li>
            <li>Papers must not exceed 4 pages in length, including references, and must adhere to the IROS formatting guidelines. Appendices that would cause the paper to exceed this limit are not permitted. Authors may include links to supplementary materials such as websites or multimedia content; however, reviewers are only expected to evaluate the content contained within the paper itself.</li>
            <li>The corresponding author must create an account on OpenReview to submit the paper. For all other co-authors, only their email addresses are required.</li>
        </ul>
    </section>


    <section id="organizers">
        <h2>Organizers</h2>
        <div class="profile">
            <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/gang.jpeg" alt="Organizer 1">
                <h3><a href="https://g-ch.github.io/" target="_blank">Gang Chen (Clarence)</a></h3>
                <p>Postdoc, TU Delft</p>
            </div>
            <div class="profile-card">
                <img src="/assets/images/people/Saray_Bakker.jpg" alt="Organizer 1">
                <h3><a href="https://saraybakker1.github.io/" target="_blank">Saray Bakker</a></h3>
                <p>PhD, TU Delft</p>
            </div>
            <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/LiamUdM.png" alt="Organizer 3">
                <h3><a href="https://liampaull.ca/" target="_blank">Liam Paull</a></h3>
                <p>Associate Professor, Université de Montréal</p>
            </div>
            <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/jia.png" alt="Organizer 4">
                <h3><a href="https://increase24.github.io/" target="_blank">Jia Zeng</a></h3>
                <p>Researcher, Shanghai AI Lab</p>
            </div>
            <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/javier.png" alt="Organizer 2">
                <h3><a href="https://autonomousrobots.nl/people/" target="_blank">Javier Alonso-Mora</a></h3>
                <p>Associate Professor, TU Delft</p>
            </div>

        </div>
    </section>

    <section id="sponsors" style="background: white; padding: 20px 0; text-align: center;">
        <h2>Sponsor</h2>
        <a href="https://en.nokov.com/" target="_blank" rel="noopener noreferrer">
          <img src="/assets/images/workshops/2025_iros/nokov.png" alt="Sponsor Logo" style="height: 80px; cursor: pointer;">
        </a>
      </section>

    <footer>
        <p>&copy; 2025 Perception and Planning for Mobile Manipulation in Changing Environments Workshop in IROS</p>
    </footer>
</body>
</html>
