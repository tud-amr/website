---
title: Perception and Planning for Mobile Manipulation in Changing Environments
venue: IROS 2025
location: Hangzhou, China
date: 2025-10-20
time: 8:00-12:00 AM
---

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Perception and Planning for Mobile Manipulation in Changing Environments</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
        }
        /* header {
            background: #333;
            color: white;
            padding: 20px;
            text-align: center;
        } */
        header {
            position: relative;
            height: auto;
            min-height: 300px;
            text-align: center;
            color: white;
        }
        .header-content {
            padding: 40px 20px;  /* Reduced padding for mobile */
        }
        .header-background {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: -1;
            display: flex;
            flex-direction: row;  /* Make sure items flow horizontally */
            justify-content: space-between;
        }
        .header-image-left {
            width: 50%;  /* Slightly less than 50% to create some gap */
            height: 100%;
            background: url('/assets/images/workshops/2025_iros/banner_1_v3.webp') no-repeat center center/cover;
            float: left;  /* Ensure left alignment */
        }
        .header-image-right {
            width: 50%;  /* Slightly less than 50% to create some gap */
            height: 100%;
            background: url('/assets/images/workshops/2025_iros/bkg2.webp') no-repeat center center/cover;
            float: right;  /* Ensure right alignment */
        }
        .overlay {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.5);
            z-index: -1;
        }
        .header-content {
            position: relative;
            padding: 100px;
        }
        nav {
            background: #444;
            padding: 10px;
            text-align: center;
            position: sticky;
            top: 0;
            z-index: 1000; /* ensures it stays above content */
        }
        nav ul {
            list-style: none;
            padding: 0;
        }
        /* nav ul li {
            display: inline;
            margin: 0 15px;
        } */
        nav ul li {
            display: inline;
            margin: 0 40px;
        }
        nav ul li a {
            color: white;
            text-decoration: none;
            font-size: 18px;
        }
        #home p, #home ul {
            width: 66%;
            margin-left: auto;
            margin-right: auto;
            /* padding-left: 20px;
            padding-right: 20px; */
        }
        #papers p, #papers ul {
            width: 66%;
            margin-left: auto;
            margin-right: auto;
            /* padding-left: 20px;
            padding-right: 20px; */
        }
        section {
            padding: 50px;
            text-align: center;
            background: white;
            margin: 20px;
            border-radius: 10px;
            box-shadow: 0px 4px 8px rgba(0,0,0,0.2);
        }
        .schedule-table {
            width: 65%;
            border-collapse: collapse;
            margin-top: 20px;
            overflow-x: auto;
            margin-left: auto;
            margin-right: auto;
            text-align: center;
            /* display: block; */
        }
        .schedule-table th, .schedule-table td {
            border: 1px solid #ddd;
            padding: 10px;
            text-align: left;
        }
        .schedule-table th {
            background-color: #333;
            color: white;
        }
        footer {
            background: #333;
            color: white;
            text-align: center;
            padding: 10px;
            position: relative;
            bottom: 0;
            width: 100%;
        }
        .profile {
            display: flex;
            align-items: center;
            justify-content: center;
            width: 66%;
            margin-left: auto;
            margin-right: auto;
            flex-wrap: wrap;
            gap: 20px;
            margin-top: 20px;
        }
        .profile-card {
            background: #fff;
            padding: 15px;
            border-radius: 10px;
            text-align: center;
            width: 320px;
            box-shadow: 0px 4px 8px rgba(0,0,0,0.2);
        }
        .profile-card img {
            width: 280;
            height: 280px;
            border-radius: 50%;
            object-fit: cover;
        }
        .profile-card a {
            text-decoration: none;
            color: #5E35B1;  /* Modern blue */
        }
        .profile-card a:hover {
            text-decoration: underline;
            /* Optional: slightly darker shade on hover */
            filter: brightness(85%);
        }
        .bio {
            display: none;
            text-align: left;
            margin-top: 10px;
        }
        .show-bio {
            color: #5E35B1;
            cursor: pointer;
        }

        ul li p {
            list-style-position: outside;
            text-indent: -10em;
            padding-left: 2em;
        }

        section ul {
            text-align: left;
            font-family: Arial, sans-serif;
            font-size: 1.2em;
        }
        

        @media screen and (max-width: 768px) {
            .header-content h1 p {
                font-size: 1.2em !important;  /* Override inline styles */
            }
            
            .header-content h1 p:nth-child(2) {
                font-size: 0.9em !important;
            }
            
            .header-content h1 p:nth-child(3) {
                font-size: 0.7em !important;
            }
            
            /* Adjust background images for mobile */
            .header-image-left, .header-image-right {
                width: 100%;
                height: 50%;
                float: none;
            }
            
            .header-background {
                flex-direction: column;  /* Stack images vertically on mobile */
            }
        }
    </style>
    <script>
        function toggleBio(id) {
            var bio = document.getElementById(id);
            if (bio.style.display === "none") {
                bio.style.display = "block";
            } else {
                bio.style.display = "none";
            }
        }
    </script>
</head>

<body>
    <header>
        <!-- <div class="header-background"></div> -->
        <div class="header-background">
            <div class="header-image-left"></div>
            <div class="header-image-right"></div>
        </div>
        <div class="overlay"></div>
        <div class="header-content">
            <h1><p style="font-size: 1.5em;">Perception and Planning for Mobile Manipulation in Changing Environments</p></h1>
            <h1><p style="font-size: 1.0em;">IROS 2025 Workshop</p></h1>
            <h1><p style="font-size: 0.8em;"> 8:00-12:00 AM, Oct. 20, 2025 </p></h1>
            <h1><p style="font-size: 0.8em;"> Venue 309, Hangzhou International Expo Center, China </p></h1>
        </div>
    </header>
    <nav>
        <ul>
            <li><a href="#home">Home</a></li>
            <li><a href="#schedule">Schedule</a></li>
            <li><a href="#speakers">Invited Speakers</a></li>
            <li><a href="#papers">Accepted Papers</a></li>
            <li><a href="#organizers">Organizers</a></li>
            <li><a href="#sponsors">Sponsor</a></li>
        </ul>
    </nav>
    <section id="home">
        <h2>About the Workshop </h2>
        <p style="text-align: left; font-size: 1.2em;">  Human workers in factories, warehouses, and hospitality settings effortlessly perceive their surroundings, adapt to continuous changes, and adjust their actions based on new assignments or environmental modifications. In contrast, mobile robots often struggle with real-time perception and task and motion planning in these changing environments, limiting their ability to function effectively in real-world scenarios.</p>
        <p style="text-align: left; font-size: 1.2em;">To advance mobile manipulation, robots must <strong>continuously perceive environmental changes</strong>—such as shifted objects, human and other robot’s activities, and unforeseen obstacles—and <strong>update their task and motion plans accordingly</strong>. However, most existing research assumes relatively static environments, limiting adaptability in practical applications.</p>
        <p style="text-align: left; font-size: 1.2em;"> <strong>Key challenges remain in:</strong></p>
            <ul>
                <li>Efficient real-time perception and modeling of dynamic environments. </li>
                <li>Task and motion planning strategies that can quickly adapt to new/changing environments while ensuring smooth and effective execution.</li>
            </ul>
        <p style="text-align: left; font-size: 1.2em;">This workshop will explore techniques for efficient environment modeling and real-time task and motion planning in dynamic environments. We will also discuss how perception and planning can be tightly integrated to improve the adaptability and robustness of mobile manipulation systems in real-world settings.</p>

    </section>
    <section id="schedule">
        <h2>Schedule</h2>
        <table class="schedule-table">
            <tr>
                <th>Time</th>
                <th>Event</th>
                <th>Comments</th>
            </tr>
            <tr>
                <td>8:50 - 9:00</td>
                <td>Opening</td>
                <td> </td>
            </tr>
            <tr>
                <td>9:00 - 9:30</td>
                <td>Speaker 1. Abhinav Valada*</td>
                <td> </td>
            </tr>
            <tr>
                <td>9:30 - 10:00</td>
                <td>Speaker 2. Niko Suenderhauf*</td>
                <td> </td>
            </tr>
            <tr>
                <td>10:00 - 10:15</td>
                <td>Spotlight Talks</td>
                <td> 1 min lightning talk </td>
            </tr>
            <tr>
                <td>10:15 - 10:45</td>
                <td>Poster Session</td>
                <td> </td>
            </tr>
            <tr>
                <td>10:45 - 11:00</td>
                <td>Coffee Break</td>
                <td> </td>
            </tr>
            <tr>
                <td>11:00 - 11:30</td>
                <td>Speaker 3. Stefan Leutenegger*</td>
                <td> </td>
            </tr>
            <tr>
                <td>11:30 - 12:00</td>
                <td>Speaker 4. Renaud Detry*</td>
                <td> </td>
            </tr>
            <!-- <tr>
                <td>10:40 - 11:05</td>
                <td>Speaker 5</td>
                <td> </td>
            </tr> -->
            <tr>
                <td>12:00 - 12:15</td>
                <td>Award and Closing Remarks</td>
                <td> </td>
            </tr>
            <tr>
                <td>12:15 - 13:00</td>
                <td>Networking and lunch </td>
                <td> </td>
            </tr>
        </table>
        <p>* The order of the speakers might change. The organizers reserve the right to make changes to the program as needed.</p>
    </section>
    <section id="speakers">
        <h2>Invited Speakers</h2>
        <p>* Ranked by the first letter of name.</p>
        <div class="profile">
            <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/valada_page.jpg" alt="Speaker 1">
                <h3><a href="https://rl.uni-freiburg.de/people/valada" target="_blank"> Abhinav Valada</a></h3>
                <p>Professor, University of Freiburg</p>
                <p class="show-bio" onclick="toggleBio('bio1')">Show Bio</p>
                <p class="bio" id="bio1">Abhinav Valada is a Full Professor (W3) at the University of Freiburg, where he directs the Robot Learning Lab. He is a member of the Department of Computer Science, the BrainLinks-BrainTools center, and a founding faculty of the ELLIS unit Freiburg. Abhinav is a DFG Emmy Noether AI Fellow, Scholar of the ELLIS Society, IEEE Senior Member, and Chair of the IEEE Robotics and Automation Society Technical Committee on Robot Learning.

                    He received his PhD (summa cum laude) working with Prof. Wolfram Burgard at the University of Freiburg in 2019, his MS in Robotics from Carnegie Mellon University in 2013, and his BTech. in Electronics and Instrumentation Engineering from VIT University in 2010. After his PhD, he worked as a Postdoctoral researcher and subsequently an Assistant Professor (W1) from 2020 to 2023. He co-founded and served as the Director of Operations of Platypus LLC from 2013 to 2015, a company developing autonomous robotic boats in Pittsburgh, and has previously worked at the National Robotics Engineering Center and the Field Robotics Center of Carnegie Mellon University from 2011 to 2014.
                    
                    Abhinav’s research lies at the intersection of robotics, machine learning, and computer vision with a focus on tackling fundamental robot perception, state estimation, and planning problems to enable robots to operate reliably in complex and diverse domains. The overall goal of his research is to develop scalable lifelong robot learning systems that continuously learn multiple tasks from what they perceive and experience by interacting with the real world. For his research, he received the IEEE RAS Early Career Award in Robotics and Automation, IROS Toshio Fukuda Young Professional Award, NVIDIA Research Award, AutoSens Most Novel Research Award, among others. Many aspects of his research have been prominently featured in wider media such as the Discovery Channel, NBC News, Business Times, and The Economic Times.</p>
            </div>
            <!-- <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/Bucher.png" alt="Speaker 2">
                <h3><a href="https://bucherb.github.io/" target="_blank">Bernadette Bucher (tentative)</a></h3>
                <p>Assistant Professor, University of Michigan</p>
                <p class="show-bio" onclick="toggleBio('bio2')">Show Bio</p>
                <p class="bio" id="bio2">I am an Assistant Professor in the Robotics Department (primary) and Computer Science and Engineering Department at University of Michigan.

                    My research interests lie in the intersection of robotics, computer vision, and machine learning. My research is on learning interpretable visual representations and estimating their uncertainty for use in downstream science and robotics tasks particularly autonomous mobile manipulation.
                    
                    Previously, I have worked at the Boston Dynamics AI Institute, NVIDIA Research, and Lockheed Martin Corporation. I received my PhD in computer science in the GRASP lab at University of Pennsylvania co-advised by Dr. Kostas Daniilidis and Dr. Nikolai Matni. I received an M.A. in Mathematics, M.A. in Economics, and B.S. in Mathematics and Economics from The University of Alabama in 2014.</p>
            </div> -->
            <!-- <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/he_wang.jpg" alt="Speaker 3">
                <h3><a href="https://hughw19.github.io/" target="_blank">He Wang</a></h3>
                <p>Assistant Professor, Peking University</p>
                <p class="show-bio" onclick="toggleBio('bio3')">Show Bio</p>
                <p class="bio" id="bio3">I am a tenure-track assistant professor in the Center on Frontiers of Computing Studies (CFCS) at Peking University. I founded and lead the Embodied Perception and InteraCtion (EPIC) Lab with the mission of developing generalizable skills and embodied multimodal large model for robots to facilitate embodied AGI.

                    I am also the director of the PKU-Galbot joint lab of Embodied AI and the BAAI center of Embodied AI. I have published more than 50 papers in top conferences and journals of computer vision, robotics, and learning, including CVPR/ICCV/ECCV/TRO/ICRA/IROS/NeurIPS/ICLR/AAAI. My pioneering work on category-level 6D pose estimation, NOCS, received the 2022 World Artificial Intelligence Conference Youth Outstanding Paper (WAICYOP) Award, and my work also received ICCV 2023 best paper finalist, ICRA 2023 outstanding manipulation paper award finalist and Eurographics 2019 best paper honorable mention.
                    
                    I serve as an associate editor of Image and Vision Computing and serve as an area chair in CVPR 2022 and WACV 2022. Prior to joining Peking University, I received my Ph.D. degree from Stanford University in 2021 under the advisory of Prof. Leonidas J.Guibas and my Bachelor's degree from Tsinghua University in 2014.</p>
            </div> -->
            <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/Lerrel Pinto.jpg" alt="Speaker 4">
                <h3><a href="https://www.lerrelpinto.com/" target="_blank">Lerrel Pinto (tentative)</a></h3>
                <p>Assistant Professor, New York University</p>
                <p class="show-bio" onclick="toggleBio('bio4')">Show Bio</p>
                <p class="bio" id="bio4">I am an Assistant Professor of Computer Science at NYU Courant and part of the CILVR group. Before that, I was at UC Berkeley for a postdoc, at CMU Robotics Institute for a PhD, and at IIT Guwahati for undergrad.

                    Research: I run the General-purpose Robotics and AI Lab (GRAIL) with the goal of getting robots to generalize and adapt in the messy world we live in. Our research focuses broadly on robot learning and decision making, with an emphasis on large-scale learning (both data and models), representation learning for sensory data, developing algorithms to model actions and behavior, reinforcement learning for adapting to new scenarios, and building open-sourced affordable robots. A talk on our recent robotics efforts is here. If you are interested in joining our lab, please read this.</p>
            </div>
            <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/niko2021.png" alt="Speaker 5">
                <h3><a href="https://nikosuenderhauf.github.io/" target="_blank">Niko Suenderhauf</a></h3>
                <p>Professor, Queensland University of Technology</p>
                <p class="show-bio" onclick="toggleBio('bio5')">Show Bio</p>
                <p class="bio" id="bio5">I am a Professor at Queensland University of Technology (QUT) in Brisbane and Deputy Director of the QUT Centre for Robotics, where I lead the Visual Understanding and Learning Program.

                    I am also Deputy Director (Research) for the ARC Research Hub in Intelligent Robotic Systems for Real-Time Asset Management (2022-2027) and was Chief Investigator of the Australian Centre for Robotic Vision 2017-2020.
                    
                    I conduct research in robotic vision and robot learning, at the intersection of robotics, computer vision, and machine learning. My research interests focus on robotic learning for manipulation, interaction and navigation, scene understanding, and the reliability of deep learning for open-set and open-world conditions.</p>
            </div>
            <!-- <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/yunzhu_li.jpg" alt="Speaker 6">
                <h3><a href="https://yunzhuli.github.io/" target="_blank">Yunzhu Li (tentative)</a></h3>
                <p>Assistant Professor, Columbia University</p>
                <p class="show-bio" onclick="toggleBio('bio6')">Show Bio</p>
                <p class="bio" id="bio6">I am an Assistant Professor of Computer Science at Columbia University.

                    Before joining Columbia, I was an Assistant Professor at UIUC CS. I also spent time as a Postdoc at the Stanford Vision and Learning Lab (SVL), working with Fei-Fei Li and Jiajun Wu. I received my PhD from the Computer Science and Artificial Intelligence Laboratory (CSAIL) at MIT, where I was advised by Antonio Torralba and Russ Tedrake, and I obtained my bachelor's degree from Peking University..</p>
            </div> -->
            <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/renaud_detry_cropped.jpeg" alt="Speaker 6">
                <h3><a href="https://renaud-detry.net/" target="_blank">Renaud Detry</a></h3>
                <p>Associate Professor, KU Leuven</p>
                <p class="show-bio" onclick="toggleBio('bio6')">Show Bio</p>
                <p class="bio" id="bio6">I am an Associate Professor of robot learning at KU Leuven in Belgium, 
                    where I hold a dual appointment in electrical and mechanical engineering (groups PSI and RAM). 
                    I sit on the steering committee of Leuven.AI, I am a member of the ELLIS Society, and I am a technical advisor for OpalAI. 
                    Formerly I was group lead for the Perception Systems group at NASA JPL, Pasadena, CA, and an Assistant Professor at UCLouvain, Belgium. I am a visiting researcher at ULiege and KTH. 
                    My research interests include robot learning and computer vision, and their application to manufacturing, agriculture, healthcare, on-orbit operations and planetary exploration. 
                    At JPL, I was machine-vision lead for the surface mission of the NASA/ESA Mars Sample Return campaign.</p>
            </div>
            <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/stefan_leutenegger_cropped.jpg" alt="Speaker 3">
                <h3><a href="https://mrl.ethz.ch/the-group/people/lab-members/stefan-leutenegger.html" target="_blank">Stefan Leutenegger</a></h3>
                <p>Associate Professor, ETH Zürich</p>
                <p class="show-bio" onclick="toggleBio('bio3')">Show Bio</p>
                <p class="bio" id="bio3">Prof. Leutenegger's field of research is the area of mobile robotics, with focus on robot navigation through potentially unknown environments. He develops algorithms and software, which allow a robot (e.g. drone) using its sensors (e.g. video) to reconstruct 3D structure as well as to categorise it with the help of modern Machine Learning (including Deep Learning). This understanding enables safe navigation through challenging environments, as well as the interaction with it (including humans).</p>
            </div>
        </div>
    </section>

    </section>
    <section id="papers">
        <h2>Accepted Papers</h2>
        <p style="text-align: left; font-size: 1.2em;"></p>
        <p style="text-align: left; font-size: 1.2em;"></p>
        <p style="text-align: left; font-size: 1.2em;">The following papers have been accepted for poster presentation and a spotlight talk at the workshop.</p> 
        <p style="text-align: left; font-size: 1.2em;">Authors should print and bring their own posters and at least one author must be present during the Poster Session. Posters discussions can be continued during the coffee break and the lunch. 
        Posters should adhere to the IROS poster guidelines (no larger than 36 inches wide and 48 inches in height, <a href="https://www.iros25.org/InstructionsOnAuthorsPresentationMaterials">link</a>). 
        The author of the accepted paper can promote their work in a 1-minute spotlight talk. The authors are requested to submit a 1-slide presentation (pdf or ppt) for their spotlight talk before the 16th of October (23:59) by sending an email to <a href="mailto:irosworkshop.pm2ce@gmail.com">irosworkshop.pm2ce@gmail.com</a>. The organizers will display the slide during the spotlight talk.</p>
        <p style="text-align: left; font-size: 1.2em;"> Accepted papers will be published on the workshop website after IROS 2025, unless otherwise specified by the authors.</p>
        
        <div class="papers-container" style="width: 66%; margin: 20px auto; text-align: left;">
            <div class="paper-item" style="margin-bottom: 20px; padding: 15px; background: #f9f9f9; border-radius: 8px; border-left: 4px solid #5E35B1;">
                <h4 style="margin: 0 0 5px 0; color: #333;">Perception and Motion Planning for Mobile Manipulation with Synthetic Generated Data in Dynamic Environments</h4>
                <p style="margin: 0 0 10px 0; color: #666; font-size: 1.0em;">Artur J. Cordeiro, Pedro A. Dias, Luís F. Rocha, Frederico M. Borgues, Manuel F. Silva, José Boaventura and João P.C. de Souza</p>
                <p class="show-bio" onclick="toggleBio('paper1')" style="color: #5E35B1; cursor: pointer; margin: 0; font-size: 0.9em;">Show Abstract</p>
                <p class="bio" id="paper1" style="display: none; margin-top: 10px; padding: 10px; background: white; border-radius: 4px; font-size: 0.95em;">
                We propose a framework that enables robotic manipulators to incorporate novel objects into their operation without manual intervention. The approach combines synthetic data generation with a perception pipeline that integrates model-based and model-free techniques, enabling grasp generation without complete 3D models and robust pose estimation from partial visual cues. The system is deployed on a mobile manipulator using a state-machine architecture that coordinates perception, navigation, and manipulation. Experiments in both controlled and unstructured environments demonstrate reliable performance across diverse objects, achieving success rates of 88.47% in grasp prediction, 98.47% in grasp reaching, 96.72% in grasp holding, and 100% in handling.
                </p>
            </div>

            <div class="paper-item" style="margin-bottom: 20px; padding: 15px; background: #f9f9f9; border-radius: 8px; border-left: 4px solid #5E35B1;">
                <h4 style="margin: 0 0 5px 0; color: #333;">A Benchmark for Multi-Modal Multi-Robot Multi-Goal Path Planning</h4>
                <p style="margin: 0 0 10px 0; color: #666; font-size: 1.0em;">Valentin N. Hartmann, Tirza Heinle, Yijiang Huang and Stelian Coros </p>
                <p class="show-bio" onclick="toggleBio('paper2')" style="color: #5E35B1; cursor: pointer; margin: 0; font-size: 0.9em;">Show Abstract</p>
                <p class="bio" id="paper2" style="display: none; margin-top: 10px; padding: 10px; background: white; border-radius: 4px; font-size: 0.95em;">
                    In many industrial robotics applications, multiple robots are working in a shared workspace to complete a set of tasks as fast as possible.
Such settings can be treated as multi-modal multi-robot multi-goal path planning problems, where each robot has to reach a set of goals. Existing approaches to this type of problem are neither optimal nor complete. We tackle this problem as a single centralized path planning problem and present planners that are probabilistically complete and asymptotically optimal. The planners plan in the composite space of all robots and are modifications of standard sampling-based planners where we introduce the required changes to work in our multi-modal, multi-robot, multi-goal setting. We validate the planners on a diverse range of problems including scenarios with various robots, planning horizons, and collaborative tasks such as handovers, and compare the planners against a suboptimal prioritized planner.
Videos and code for the planners and the benchmark is available at <a href="https://github.com/vhartman/multirobot-pathplanning-benchmark">https://github.com/vhartman/multirobot-pathplanning-benchmark</a>.
                </p>
            </div>

            <div class="paper-item" style="margin-bottom: 20px; padding: 15px; background: #f9f9f9; border-radius: 8px; border-left: 4px solid #5E35B1;">
                <h4 style="margin: 0 0 5px 0; color: #333;">Accelerated Multi-Modal Motion Planning Using Context-Conditioned Diffusion Models</h4>
                <p style="margin: 0 0 10px 0; color: #666; font-size: 1.0em;">Edward Sandra, Lander Vanroye, Dries Dirckx, Ruben Cartuyvels, Jan Swevers and Wilm Decré </p>
                <p class="show-bio" onclick="toggleBio('paper3')" style="color: #5E35B1; cursor: pointer; margin: 0; font-size: 0.9em;">Show Abstract</p>
                <p class="bio" id="paper3" style="display: none; margin-top: 10px; padding: 10px; background: white; border-radius: 4px; font-size: 0.95em;">
                Classical methods in robot motion planning, such as sampling-based and optimization-based methods, often struggle with scalability towards higher-dimensional state spaces and complex environments. Diffusion models, known for their capability to learn complex, high-dimensional and multi-modal data distributions, provide a promising alternative when applied to motion planning problems and have already shown interesting results. However, most of the current approaches train their model for a single environment, limiting their generalization to unseen environments. The techniques that do train a model for multiple environments rely on a specific camera to provide the model with the necessary environmental information. 
To effectively adapt to diverse scenarios without the need for retraining, this research proposes Context-Aware Motion Planning Diffusion (CAMPD).
CAMPD leverages a classifier-free denoising probabilistic diffusion model, conditioned on sensor-agnostic contextual information.
An attention mechanism, integrated in the well-known U-Net architecture, conditions the model on an arbitrary number of contextual parameters. CAMPD is evaluated on a 7-DoF robot manipulator and benchmarked against state-of-the-art approaches on real-world tasks, demonstrating its adaptability to changing and unseen environments while generating high-quality, multi-modal trajectories at a fraction of the time required by existing methods.
                </p>
            </div>

            <div class="paper-item" style="margin-bottom: 20px; padding: 15px; background: #f9f9f9; border-radius: 8px; border-left: 4px solid #5E35B1;">
                <h4 style="margin: 0 0 5px 0; color: #333;">Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots</h4>
                <p style="margin: 0 0 10px 0; color: #666; font-size: 1.0em;">Wei Cui, Haoyu Wang, Jiaru Zhong, Wenkang Qin, Yijie Guo, Gang Han, Wen Zhao, Jiahang Cao, Zhang Zhang, Jingkai SUN, Pihai Sun, Shuai Shi, Botuo Jiang, Jiahao Ma, Jiaxu Wang, Hao Cheng, Zhichao Liu, Yang Wang, Zheng Zhu, Guan Huang, Lingfeng Zhang, Jun Ma, Junwei Liang, Renjing Xu, Jian Tang and Qiang Zhang</p>
                <p class="show-bio" onclick="toggleBio('paper4')" style="color: #5E35B1; cursor: pointer; margin: 0; font-size: 0.9em;">Show Abstract</p>
                <p class="bio" id="paper4" style="display: none; margin-top: 10px; padding: 10px; background: white; border-radius: 4px; font-size: 0.95em;">
                As the most promising general-purpose robots, humanoid robots have witnessed rapid development. The visual perception module serves as a crucial component within their system, providing essential information for key tasks such as mobile manipulation.
Therefore, this paper proposes a perception framework centered on semantic occupancy, termed Humanoid Occupancy, enhancing perception capabilities by optimized scene representation and efficient data fusion. Specifically, we design a sensor arrangement scheme, develop the first panoramic occupancy dataset for humanoid robots, and propose a lightweight multi-modal temporal feature fusion network which can present the dynamic environments precisely. Furthermore, this perception method has been deployed on the Tienkung humanoid robot platform, demonstrating its superior environmental perception and navigation performance in changing environments.
                </p>
            </div>

            <div class="paper-item" style="margin-bottom: 20px; padding: 15px; background: #f9f9f9; border-radius: 8px; border-left: 4px solid #5E35B1;">
                <h4 style="margin: 0 0 5px 0; color: #333;">Autonomous Robotic Manipulation with a Clutter-Aware Pushing Policy</h4>
                <p style="margin: 0 0 10px 0; color: #666; font-size: 1.0em;">Sanraj Lachhiramka, Pradeep J, Archanaa A. Chandaragi, Arjun Achar and Shikha Tripathi </p>
                <p class="show-bio" onclick="toggleBio('paper5')" style="color: #5E35B1; cursor: pointer; margin: 0; font-size: 0.9em;">Show Abstract</p>
                <p class="bio" id="paper5" style="display: none; margin-top: 10px; padding: 10px; background: white; border-radius: 4px; font-size: 0.95em;">
                This work addresses the challenge of grasping a target object in cluttered environments, even when it is partially visible or fully occluded. The proposed approach enables the manipulator to learn a sequence of strategic non-prehensile (pushing) actions to rearrange the scene and make the target object graspable. Our pipeline integrates a deep reinforcement learning (DRL) agent for pushing with the GR-ConvNet model for grasp prediction. When the object is considered ungraspable, a Soft Actor-Critic (SAC) model guides optimal pushing actions.  A key contribution is a novel pixel-wise clutter map, which is integrated directly into the agent's state representation to provide an explicit, quantitative measure of environmental clutter. This clutter-aware state representation guides the decision-making process, leading to more efficient policies. Experimental results demonstrate that incorporating the clutter map significantly improves performance, reducing the number of actions required to complete the task by approximately 25%. The system generalizes well to diverse objects and transfers directly from simulation to hardware without requiring additional training for real-world deployment.
                </p>
            </div>
        
            <div class="paper-item" style="margin-bottom: 20px; padding: 15px; background: #f9f9f9; border-radius: 8px; border-left: 4px solid #5E35B1;">
                <h4 style="margin: 0 0 5px 0; color: #333;">Jacobian-Guided Active Learning for Gaussian Process-Based Inverse Kinematics</h4>
                <p style="margin: 0 0 10px 0; color: #666; font-size: 1.0em;">Shibao Yang, Pengcheng Liu and Nick Pears</p>
                <p class="show-bio" onclick="toggleBio('paper6')" style="color: #5E35B1; cursor: pointer; margin: 0; font-size: 0.9em;">Show Abstract</p>
                <p class="bio" id="paper6" style="display: none; margin-top: 10px; padding: 10px; background: white; border-radius: 4px; font-size: 0.95em;">
                Frequent replanning in dynamically changing environments often pushes robot manipulators towards singular configurations and joint limits, causing traditional inverse kinematics (IK) solvers to fail and hindering adaptability. We address this with an enhanced Gaussian Process IK (GP-IK) framework that uses a Jacobian-guided acquisition strategy for robust planning. This method adapts its exploration-exploitation balance in real-time based on local sensitivity and mechanical constraints, ensuring the planner can find reliable solutions even near manipulator limits. By enabling robust performance in challenging configurations, our approach allows for a tighter integration of perception and planning, fostering more adaptive and resilient robots, as demonstrated on a 7-DOF Franka robot.
                </p>
            </div>

            <div class="paper-item" style="margin-bottom: 20px; padding: 15px; background: #f9f9f9; border-radius: 8px; border-left: 4px solid #5E35B1;">
                <h4 style="margin: 0 0 5px 0; color: #333;">Dynamic Objects Relocalization in Changing Environments with Flow Matching</h4>
                <p style="margin: 0 0 10px 0; color: #666; font-size: 1.0em;">Francesco Argenziano, Miguel Saavedra-Ruiz, Sacha Morin, Daniele Nardi and Liam Paull </p>
                <p class="show-bio" onclick="toggleBio('paper7')" style="color: #5E35B1; cursor: pointer; margin: 0; font-size: 0.9em;">Show Abstract</p>
                <p class="bio" id="paper7" style="display: none; margin-top: 10px; padding: 10px; background: white; border-radius: 4px; font-size: 0.95em;">
                Task and motion planning are long-standing challenges in robotics, especially when robots have to deal with dynamic environments exhibiting long-term dynamics, such as households or warehouses. In these environments, long-term dynamics mostly stem from human activities, since previously detected objects can be moved or removed from the scene. This adds the necessity to find such objects again before completing the designed task, increasing the risk of failure due to missed relocalizations. However, in these settings, the nature of such human-object interactions is often overlooked, despite being governed by common habits and repetitive patterns. Our conjecture is that these cues can be exploited to recover the most likely objects' positions in the scene, helping to address the problem of unknown relocalization in changing environments. To this end we propose FlowMaps a model based on Flow Matching that is able to infer multimodal object locations over space and time. Our results present statistical evidence to support our hypotheses, opening the way to more complex applications of our approach. The code is publically available at <a href="https://github.com/Fra-Tsuna/flowmaps">https://github.com/Fra-Tsuna/flowmaps</a>.
                </p>
            </div>

            <div class="paper-item" style="margin-bottom: 20px; padding: 15px; background: #f9f9f9; border-radius: 8px; border-left: 4px solid #5E35B1;">
                <h4 style="margin: 0 0 5px 0; color: #333;">Open-Vocabulary and Semantic-Aware Reasoning for Search and Retrieval of Objects in Dynamic and Concealed Spaces</h4>
                <p style="margin: 0 0 10px 0; color: #666; font-size: 1.0em;">Rohit Menon, Yasmin Schmiede, Maren Bennewitz and Hermann Blum</p>
                <p class="show-bio" onclick="toggleBio('paper8')" style="color: #5E35B1; cursor: pointer; margin: 0; font-size: 0.9em;">Show Abstract</p>
                <p class="bio" id="paper8" style="display: none; margin-top: 10px; padding: 10px; background: white; border-radius: 4px; font-size: 0.95em;">
                We present an open-vocabulary framework that combines spatial, semantic, and geometric reasoning to solve a highly relevant, yet under-studied problem: Given an (outdated) map of an environment, how can a robot efficiently retrieve relocated or unmapped items? By unifying spatial cues about proximity and topology, semantic priors on typical placements, and geometric constraints that rule out infeasible locations, particularly within concealed spaces, our approach finds objects even when they are relocated or hidden in drawers or cabinets.
We further propose in-situ viewpoint planning to model new objects for manipulation, and to add the object to our dynamic 3D scene graph.
We validate our framework through extensive real-world trials on the Stretch SE3 mobile manipulator, evaluating search and retrieval in various conditions. Results demonstrate robust navigation (100%) and open-space detection (100%), with semantic-geometric reasoning reducing concealed space search time by 68% versus semantic-only approaches. 
Implemented on a low-cost, compact mobile manipulator, our solution combines sophisticated cognitive capabilities with practical deployability, representing a significant step toward accessible service robots for everyday homes.
                </p>
            </div>

            <div class="paper-item" style="margin-bottom: 20px; padding: 15px; background: #f9f9f9; border-radius: 8px; border-left: 4px solid #5E35B1;">
                <h4 style="margin: 0 0 5px 0; color: #333;">HANDO: Hierarchical Autonomous Navigation and Dexterous Omni-loco-manipulation</h4>
                <p style="margin: 0 0 10px 0; color: #666; font-size: 1.0em;">Jingyuan Sun, Chaoran Wang, Mingyu Zhang, Cui Miao, Hongyu Ji, Zihan Qu, Sun Han, Bing Wang and Qingyi Si </p>
                <p class="show-bio" onclick="toggleBio('paper9')" style="color: #5E35B1; cursor: pointer; margin: 0; font-size: 0.9em;">Show Abstract</p>
                <p class="bio" id="paper9" style="display: none; margin-top: 10px; padding: 10px; background: white; border-radius: 4px; font-size: 0.95em;">
                Seamless loco-manipulation in unstructured environments requires robots to leverage autonomous exploration alongside whole-body control for physical interaction. In this work, we introduce HANDO (Hierarchical Autonomous Navigation and Dexterous Omni-loco-manipulation), a two-layer framework designed for legged robots equipped with manipulators to perform human-centered mobile manipulation tasks. The first layer utilizes a goal-conditioned autonomous exploration policy to guide the robot to semantically specified
targets, such as a black office chair in a dynamic environment. The second layer employs a unified whole-body loco-manipulation policy to coordinate the arm and legs for precise interaction tasks—for example, handing a drink to a person seated on the chair. We have conducted an initial deployment of the navigation module, and will continue to pursue finer-grained deployment of whole-body loco-manipulation.
                </p>
            </div>

            <div class="paper-item" style="margin-bottom: 20px; padding: 15px; background: #f9f9f9; border-radius: 8px; border-left: 4px solid #5E35B1;">
                <h4 style="margin: 0 0 5px 0; color: #333;">T-FunS3D: Task-Driven Hierarchical Open-Vocabulary 3D Functionality Segmentation via Vision-Language Models</h4>
                <p style="margin: 0 0 10px 0; color: #666; font-size: 1.0em;">Jingkun Feng and Reza Sabzevari</p>
                <p class="show-bio" onclick="toggleBio('paper10')" style="color: #5E35B1; cursor: pointer; margin: 0; font-size: 0.9em;">Show Abstract</p>
                <p class="bio" id="paper10" style="display: none; margin-top: 10px; padding: 10px; background: white; border-radius: 4px; font-size: 0.95em;">
                Open-vocabulary 3D functionality segmentation enables robots to localize functional object components in 3D scenes. It is a challenging task that requires spatial understanding and task interpretation. Current open-vocabulary 3D segmentation methods primarily focus on object-level recognition, while scene-wide part segmentation methods attempt to segment the entire scene exhaustively, making them highly resource-intensive. However, such significant computational and storage capacities are typically not accessible on the majority of mobile robots. To address this challenge, we introduce T-FunS3D, a task-driven hierarchical open-vocabulary 3D functionality segmentation method that provides actionable perception for robotic applications. Given a task description, T-FunS3D identifies the most relevant instances in an open-vocabulary scene graph and extracts their functional components. Experiments on SceneFun3D demonstrate that T-FunS3D outperforms baseline methods in open-vocabulary 3D functionality segmentation, while achieving faster runtime and reduced memory usage.
                </p>
            </div>

            <div class="paper-item" style="margin-bottom: 20px; padding: 15px; background: #f9f9f9; border-radius: 8px; border-left: 4px solid #5E35B1;">
                <h4 style="margin: 0 0 5px 0; color: #333;">Air-ground Robotic Collaborative Framework for Autonomous Cramped-space Post-earthquake Search</h4>
                <p style="margin: 0 0 10px 0; color: #666; font-size: 1.0em;">Ruiyang Yang, Ming Xue, Yue Zeng, Zihao Yang, Yudong Fang, Jixing Yang, Yongqiang Liu, Xiaohui Huang and Jingya Liu </p>
                <p class="show-bio" onclick="toggleBio('paper11')" style="color: #5E35B1; cursor: pointer; margin: 0; font-size: 0.9em;">Show Abstract</p>
                <p class="bio" id="paper11" style="display: none; margin-top: 10px; padding: 10px; background: white; border-radius: 4px; font-size: 0.95em;">
                After an earthquake, employing robots for search and rescue has become a prevailing trend. However, complex and changing environments, such as composite ruins and limited-space caves, present significant challenges for personnel to approach, making effective plans difficult, and a single type of robot is insufficient given constraints on weight, size, and endurance. To address these issues, an air-ground robotic collaborative framework is proposed. Specifically, a quadruped robot-based recessed helipad structural component with lateral clamping arms is designed for safely and stably on-board manipulation. The quadruped robot carries the drone to the vicinity of the narrow space, and a deep-learning-based perception algorithm is applied to determine whether the drone autonomously launches and continues the inspection. The drone can finally return on its own. The framework is verified through an authentic earthquake training environment, providing promising applications for unmanned search and rescue.
                </p>
            </div>

            <div class="paper-item" style="margin-bottom: 20px; padding: 15px; background: #f9f9f9; border-radius: 8px; border-left: 4px solid #5E35B1;">
                <h4 style="margin: 0 0 5px 0; color: #333;">Perception-Driven Adaptive Mobile Manipulation for Autonomous Container Unloading</h4>
                <p style="margin: 0 0 10px 0; color: #666; font-size: 1.0em;">Maria S. Lopes, João Pedro C. de Souza, Pedro Costa, José A. Beça and Manuel F. Silva </p>
                <p class="show-bio" onclick="toggleBio('paper12')" style="color: #5E35B1; cursor: pointer; margin: 0; font-size: 0.9em;">Show Abstract</p>
                <p class="bio" id="paper12" style="display: none; margin-top: 10px; padding: 10px; background: white; border-radius: 4px; font-size: 0.95em;">
                Maritime container unloading is a physically demanding task often carried out under challenging conditions, motivating automation. However, automating this process is complex due to each shipment's unpredictable sizes and quantities. Existing solutions tend to be task-specific, with hardware and software designed exclusively for container handling, which restricts their adaptability. We propose a modular framework that empowers a versatile mobile manipulator with standard sensors and actuators to unload containers autonomously. This system integrates real-time perception and adaptive task execution, enabling it to react effectively to task progress and feedback. Simulation results indicate that the system can unload an entire container without collisions, and individual modules have been successfully tested in a laboratory setting, with full pipeline validation currently in progress.
                </p>
            </div>

            <div class="paper-item" style="margin-bottom: 20px; padding: 15px; background: #f9f9f9; border-radius: 8px; border-left: 4px solid #5E35B1;">
                <h4 style="margin: 0 0 5px 0; color: #333;">U-LAG: Uncertainty-Aware, Lag-Adaptive Goal Retargeting for Robotic Manipulation</h4>
                <p style="margin: 0 0 10px 0; color: #666; font-size: 1.0em;">Anamika J. H. and Anujith Muraleedharan </p>
                <p class="show-bio" onclick="toggleBio('paper13')" style="color: #5E35B1; cursor: pointer; margin: 0; font-size: 0.9em;">Show Abstract</p>
                <p class="bio" id="paper13" style="display: none; margin-top: 10px; padding: 10px; background: white; border-radius: 4px; font-size: 0.95em;">
                Robots often grasp with observations that are delayed, noisy, or stale. We present U-LAG, a mid-execution goal-retargeting layer that leaves the low-level controller unchanged while re-aiming task goals (pre-grasp hover, grasp, post-lift) as fresh observations arrive. Unlike visual servoing or online re-planning that redesign control or regenerate trajectories, U-LAG treats in-flight goal re-aiming as a pluggable module between perception and control. Our main technical contribution is UAR-PF, an uncertainty-aware retargeter that maintains a belief over object pose under sensing lag and selects goals that maximize expected capture, evaluated against ICP and nearest-geometry baselines within the same interface. We instantiate a reproducible Shift×Lag stress test in PyBullet/PandaGym for pick and place, where the object undergoes abrupt in-plane shifts while synthetic perception lag is injected during approach. Across 0–10 cm shifts and 0–400 ms lags, UAR-PF degrades gracefully relative to a no-retarget baseline and achieves higher success with modest end-effector travel and few aborts; simple operational safeguards further improve stability.
                </p>
            </div>
        </div>
        
        
        <!-- <p style="text-align: left; font-size: 1.2em;"> We invite researchers and industry professionals to submit short papers of up to four pages for presentation at our workshop on “<b>Perception and Planning for Mobile Manipulation in Changing Environments</b>”. 
            The workshop will take place at IROS 2025, Hangzhou, China, on 20th October 2025 (8:00-12:00 AM). 
            All submissions will undergo <b>single-blind peer review</b> by experts in the field. 
            Accepted submissions will be presented during <b>dedicated poster sessions</b> at the workshop and in spotlight presentations. 
            Authors are responsible for <b>printing and bringing their own posters</b> and <b>at least one author must be present</b> during their poster presentation. 
            Accepted abstracts will be <b>published on the workshop website after IROS 2025</b>, unless otherwise specified by the authors.
            All submissions should be uploaded via <a href=https://openreview.net/group?id=IEEE.org/IROS/2025/Workshop/PM2CE>OpenReview</a>.</p>
            <p style="text-align: left; font-size: 1.2em;"> Papers should be submitted before <b><del>Aug. 25</del> Sept. 7, 2025, 23.59 PM UTC-0</b>. The notification of acceptance will be sent around <del>Sept. 1,</del> Sept. 14 2025.</p>
            <p style="text-align: left; font-size: 1.2em;"> We offer a <b>300 USD award</b> for the best workshop paper.</p>
            <p style="text-align: left; font-size: 1.2em;"><b>Topics of interest include, but are not limited to:</b></p>
        <ul>
            <li>Long-term dynamic environment/object modeling </li>
            <li>Open-world environment modeling</li>
            <li>Task and motion planning for manipulation or mobile manipulation</li>
            <li>Motion planning for single or multiple mobile manipulators</li>
            <li>Inter-robot communication</li>
            <li>Human-robot/robot-robot interaction</li>
            <li>Multi-robot coordination/collaboration in manipulation tasks</li>
            <li>Mobile manipulation systems</li>
        </ul>
        -->
    </section>


    <section id="organizers">
        <h2>Organizers</h2>
        <div class="profile">
            <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/gang.jpeg" alt="Organizer 1">
                <h3><a href="https://g-ch.github.io/" target="_blank">Gang Chen (Clarence)</a></h3>
                <p>Postdoc, TU Delft</p>
            </div>
            <div class="profile-card">
                <img src="/assets/images/people/Saray_Bakker.jpg" alt="Organizer 1">
                <h3><a href="https://saraybakker1.github.io/" target="_blank">Saray Bakker</a></h3>
                <p>PhD, TU Delft</p>
            </div>
            <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/LiamUdM.png" alt="Organizer 3">
                <h3><a href="https://liampaull.ca/" target="_blank">Liam Paull</a></h3>
                <p>Associate Professor, Université de Montréal</p>
            </div>
            <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/jia.png" alt="Organizer 4">
                <h3><a href="https://increase24.github.io/" target="_blank">Jia Zeng</a></h3>
                <p>Researcher, Shanghai AI Lab</p>
            </div>
            <div class="profile-card">
                <img src="/assets/images/workshops/2025_iros/people/javier.png" alt="Organizer 2">
                <h3><a href="https://autonomousrobots.nl/people/" target="_blank">Javier Alonso-Mora</a></h3>
                <p>Associate Professor, TU Delft</p>
            </div>

        </div>
    </section>

    <section id="sponsors" style="background: white; padding: 20px 0; text-align: center;">
        <h2>Sponsor</h2>
        <a href="https://en.nokov.com/" target="_blank" rel="noopener noreferrer">
          <img src="/assets/images/workshops/2025_iros/nokov.png" alt="Sponsor Logo" style="height: 80px; cursor: pointer;">
        </a>
      </section>

    <footer>
        <p>&copy; 2025 Perception and Planning for Mobile Manipulation in Changing Environments Workshop in IROS</p>
    </footer>
</body>
</html>
